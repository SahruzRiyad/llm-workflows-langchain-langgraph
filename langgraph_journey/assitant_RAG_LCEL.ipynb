{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahruz Riyad\\AppData\\Local\\Temp\\ipykernel_2972\\473586767.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "chat = ChatOpenAI(temperature=0.0, model_name=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye! See you soon.. \n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = input(\"Enter your question: if you want to quit the chat, write exit\")\n",
    "\n",
    "    if question != \"exit\":\n",
    "        print(chat.invoke(question).content)\n",
    "    else:\n",
    "        print(\"Goodbye! See you soon.. \") \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response to name introduction: Nice to meet you, Saharuggaman Reyad! How can I assist you today?\n",
      "Response to memory check: Yes, your name is Saharuggaman Reyad.\n",
      "Response to memory recall: Your name is Saharuggaman Reyad.\n",
      "Memory content: [HumanMessage(content='My name is Saharuggaman Reyad.', additional_kwargs={}, response_metadata={}), AIMessage(content='Nice to meet you, Saharuggaman Reyad! How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Yes, your name is Saharuggaman Reyad.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Saharuggaman Reyad.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize OpenAI model (use your API key if necessary)\n",
    "chat = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "\n",
    "# Initialize memory to store conversation context\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "\n",
    "# Define prompt template that explicitly uses previous conversation context\n",
    "prompt = ChatPromptTemplate.from_template(\"Q: {question}\\n{chat_history}\\nA:\")\n",
    "\n",
    "# Create the LLMChain using prompt, model, and memory\n",
    "conversation_chain = LLMChain(llm=chat, prompt=prompt, memory=memory)\n",
    "\n",
    "# Function to simulate conversation with manual context passing\n",
    "def conversation():\n",
    "    # First message to introduce your name\n",
    "    response_1 = conversation_chain.run({\"question\": \"My name is Saharuggaman Reyad.\"})\n",
    "    print(f\"Response to name introduction: {response_1}\")\n",
    "\n",
    "    # Ask if it remembers your name\n",
    "    response_2 = conversation_chain.run({\"question\": \"Can you remember my name?\"})\n",
    "    print(f\"Response to memory check: {response_2}\")\n",
    "\n",
    "    # Follow up to ensure memory is working\n",
    "    response_3 = conversation_chain.run({\"question\": \"What is my name?\"})\n",
    "    print(f\"Response to memory recall: {response_3}\")\n",
    "\n",
    "    # Check the memory content explicitly (useful for debugging)\n",
    "    print(\"Memory content:\", memory.buffer)\n",
    "\n",
    "# Run the conversation simulation\n",
    "conversation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='My name is Saharuggaman Reyad.', additional_kwargs={}, response_metadata={}), AIMessage(content='Nice to meet you, Saharuggaman Reyad! How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an artificial intelligence, I don't have the ability to remember or recall personal data unless it's provided in the context of the conversation. I am designed to respect user's privacy and confidentiality.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Saharuggaman Reyad.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.run(\"What is my name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an artificial intelligence, I don't have an age or a creation date.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.run(\"My age is 26, what is yours? when do you created?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You mentioned earlier that your age is 26.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.run({\"question\": \"what is my age?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your favorite color is blue.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.run({\"question\": \"my favorite color is blue?\"})\n",
    "conversation_chain.run({\"question\": \"what is my favourite color?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
