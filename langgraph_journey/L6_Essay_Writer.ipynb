{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = tavily_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator, sqlite3\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
    "\n",
    "conn = sqlite3.connect(\"checkpont.sqlite\", check_same_thread=False)\n",
    "memory = SqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: str \n",
    "    plan: str \n",
    "    draft: str\n",
    "    critique: str \n",
    "    content: List[str]\n",
    "    revision_number: int \n",
    "    max_revisions: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay. \\\n",
    "Write such an outline for the user provided topic. Give an outline of the essay along with any relevant notes \\\n",
    "or instructions for the sections.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays.\\\n",
    "Generate the best essay possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
    "Utilize all the information below as needed: \n",
    "\n",
    "------\n",
    "\n",
    "{content}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following essay. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as outlined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT),\n",
    "        HumanMessage(content= state['task'])\n",
    "    ]\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "    return {\"plan\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_plan_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily_tool.run(q)\n",
    "\n",
    "        for r in response:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_state: AgentState = {\n",
    "    \"task\": \"Write a story about a cat\",\n",
    "    \"plan\": \"1. Introduction. 2. Middle. 3. Ending.\",\n",
    "    \"draft\": \"\",\n",
    "    \"critique\": \"\",\n",
    "    \"content\": [\"Once upon a time...\"],\n",
    "    \"revision_number\": 1,\n",
    "    \"max_revisions\": 3,\n",
    "}\n",
    "\n",
    "research_plan_node(example_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join(state['content'] or [])\n",
    "    user_message = HumanMessage(\n",
    "        content = f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\"\n",
    "    )\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content= WRITER_PROMPT.format(content=content)\n",
    "        ),\n",
    "        user_message\n",
    "    ]\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content,\n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage( content=REFLECTION_PROMPT),\n",
    "        HumanMessage(content=state['draft'])\n",
    "    ]\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "    return {\"critique\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_critique_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content= RESEARCH_CRITIQUE_PROMPT),\n",
    "        HumanMessage(content= state['critique'])\n",
    "    ])\n",
    "\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily_tool.run(q)\n",
    "        for r in response:\n",
    "            content.append(r['content'])\n",
    "    \n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState):\n",
    "    return state['revision_number'] <= state['max_revisions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2079a20fa10>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_node(\"planner\",plan_node)\n",
    "builder.add_node(\"generate\",generation_node)\n",
    "builder.add_node(\"reflect\",reflection_node)\n",
    "builder.add_node(\"research_plan\",research_plan_node)\n",
    "builder.add_node(\"research_critique\",research_critique_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2079a20fa10>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.set_entry_point(\"planner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2079a20fa10>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    should_continue,\n",
    "    {True: \"reflect\", False: END}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2079a20fa10>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_edge(\"planner\",\"research_plan\")\n",
    "builder.add_edge(\"research_plan\",\"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\",\"research_critique\")\n",
    "builder.add_edge(\"research_critique\",\"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"planner\": {\n",
      "        \"plan\": \"**Title: Understanding the Differences Between LangChain and LangSmith**\\n\\n**I. Introduction**\\n   - Brief introduction to the importance of language processing tools in modern technology.\\n   - Explanation of the growing landscape of language models and frameworks.\\n   - Introduction to LangChain and LangSmith as two prominent tools in this domain.\\n\\n**II. Overview of LangChain**\\n   - **A. Definition and Purpose**\\n     - Define LangChain and its primary objectives.\\n     - Discuss its role in natural language processing (NLP) and artificial intelligence (AI).\\n   - **B. Key Features**\\n     - Highlight the main features and capabilities of LangChain.\\n     - Discuss its architecture and how it integrates with other systems.\\n   - **C. Use Cases**\\n     - Provide examples of applications and industries where LangChain is commonly used.\\n     - Discuss its effectiveness in various NLP tasks.\\n\\n**III. Overview of LangSmith**\\n   - **A. Definition and Purpose**\\n     - Define LangSmith and its primary objectives.\\n     - Discuss its role in NLP and AI, contrasting it with LangChain.\\n   - **B. Key Features**\\n     - Highlight the main features and capabilities of LangSmith.\\n     - Discuss its architecture and unique selling points.\\n   - **C. Use Cases**\\n     - Provide examples of applications and industries where LangSmith is commonly used.\\n     - Discuss its effectiveness in various NLP tasks.\\n\\n**IV. Comparative Analysis**\\n   - **A. Core Differences**\\n     - Compare and contrast the core functionalities of LangChain and LangSmith.\\n     - Discuss differences in architecture, integration, and scalability.\\n   - **B. Performance and Efficiency**\\n     - Analyze the performance metrics of both tools in different scenarios.\\n     - Discuss efficiency in terms of processing speed, accuracy, and resource utilization.\\n   - **C. User Experience and Accessibility**\\n     - Compare the user interfaces and ease of use for developers and end-users.\\n     - Discuss the learning curve and community support for each tool.\\n\\n**V. Advantages and Disadvantages**\\n   - **A. LangChain**\\n     - List the advantages and potential drawbacks of using LangChain.\\n   - **B. LangSmith**\\n     - List the advantages and potential drawbacks of using LangSmith.\\n\\n**VI. Conclusion**\\n   - Summarize the key points discussed in the essay.\\n   - Provide insights into which tool might be more suitable for specific needs or industries.\\n   - Offer a forward-looking perspective on the evolution of language processing tools.\\n\\n**VII. References**\\n   - Include a list of references and resources for further reading on LangChain and LangSmith.\\n\\n**Notes/Instructions:**\\n- Ensure that the essay remains objective and fact-based, avoiding any bias towards either tool.\\n- Use technical language appropriate for an audience familiar with NLP and AI concepts.\\n- Include diagrams or tables if necessary to illustrate differences in architecture or performance metrics.\\n- Consider including quotes or insights from industry experts to add depth to the analysis.\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"research_plan\": {\n",
      "        \"content\": [\n",
      "            \"In fact, its orchestration capabilities allow for seamless model switching and dynamic load management, which can be critical when dealing with high-traffic environments where performance bottlenecks can be costly. In short, while LangChain excels at managing and scaling model workflows, LangSmith is designed for when you need deep visibility and control over large, complex AI systems in production. Both are highly valuable but serve different roles depending on your AI project's needs. [...] LangChain is designed for developers who are creating LLM-based applications from scratch. Its intuitive setup lets users quickly deploy LLM workflows without getting bogged down in complex configurations. Meanwhile, LangSmith was built so that data scientists could refine and optimize deployed workflows. These users typically understand how AI models work and can leverage LangSmith's advanced features to monitor, debug, and improve performance metrics in production. [...] Now, let's talk about resources because, in AI, efficiency can make or break your project. LangChain is typically lighter on hardware and computing resources. You don't need an overly complex setup to get it working, and in cloud environments, it's generally cost-effective since it focuses on managing workflows rather than deep system diagnostics. On the flip side, LangSmith can be more resource-intensive, especially in scenarios where you're monitoring and orchestrating multiple large models.\",\n",
      "            \"Why This Comparison Matters\\nHere\\u2019s the deal: LangChain and LangSmith aren\\u2019t just competing tools\\u2014they serve fundamentally different purposes. In my experience, LangChain is like a Swiss Army knife for building and orchestrating LLM applications. On the other hand, LangSmith is your precision toolkit when you need to debug, evaluate, and monitor those workflows. [...] Here\\u2019s the deal: if you\\u2019re designing and deploying workflows, LangChain is your foundation. It\\u2019s flexible, intuitive, and designed to help you build from the ground up. Personally, I\\u2019ve used it to create pipelines that span everything from summarization to decision-making. On the other hand, LangSmith shines when it\\u2019s time to refine. Debugging, monitoring, and evaluating\\u2014this is where LangSmith takes the spotlight. [...] | Learning Curve | Moderate: Requires understanding how LLMs work. | Minimal if you\\u2019re familiar with LangChain or LLM pipelines. |\\nDiving Deeper: Core Features\\nHere\\u2019s the deal: LangChain is like building the entire car, while LangSmith is your diagnostic tool to ensure that car runs smoothly.\\nLangChain: The Builder\\u2019s Dream\",\n",
      "            \"LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities. By understanding the differences, pros, and cons of each tool, developers can make informed decisions about which one to use for their specific needs.\",\n",
      "            \"Why This Comparison Matters\\nHere\\u2019s the deal: LangChain and LangSmith aren\\u2019t just competing tools\\u2014they serve fundamentally different purposes. In my experience, LangChain is like a Swiss Army knife for building and orchestrating LLM applications. On the other hand, LangSmith is your precision toolkit when you need to debug, evaluate, and monitor those workflows. [...] Here\\u2019s the deal: if you\\u2019re designing and deploying workflows, LangChain is your foundation. It\\u2019s flexible, intuitive, and designed to help you build from the ground up. Personally, I\\u2019ve used it to create pipelines that span everything from summarization to decision-making. On the other hand, LangSmith shines when it\\u2019s time to refine. Debugging, monitoring, and evaluating\\u2014this is where LangSmith takes the spotlight. [...] If you\\u2019re building from the ground up, LangChain is your best bet. It\\u2019s flexible, scalable, and designed for developers. But when it\\u2019s time to troubleshoot or improve your workflows, LangSmith steps in with tools to analyze, debug, and optimize.\\nFinal Note on Audiences\\nLangChain speaks to developers crafting solutions. LangSmith is a lifeline for those of us knee-deep in troubleshooting. Both are indispensable depending on where you are in your pipeline\\u2019s lifecycle.\",\n",
      "            \"LangChain vs. LangSmith: A Detailed Comparison\\nBoth tools are powerful, but they serve different purposes. Here's a breakdown of their core functionalities to help you understand which will help you the most.\\u00a0\\nFeature\\nLangChain\\nLangSmith\\nPrimary Use Case\\nBuilding and orchestrating complex LLM workflows.\\nDebugging, monitoring, and evaluating LLM workflows.\\nStrengths\\nMulti-step pipelines, agent-based systems, modular design.\\nTracing, error handling, performance monitoring, and evaluation metrics. [...] LangChain is designed for developers who are creating LLM-based applications from scratch. Its intuitive setup lets users quickly deploy LLM workflows without getting bogged down in complex configurations. Meanwhile, LangSmith was built so that data scientists could refine and optimize deployed workflows. These users typically understand how AI models work and can leverage LangSmith's advanced features to monitor, debug, and improve performance metrics in production. [...] In fact, its orchestration capabilities allow for seamless model switching and dynamic load management, which can be critical when dealing with high-traffic environments where performance bottlenecks can be costly. In short, while LangChain excels at managing and scaling model workflows, LangSmith is designed for when you need deep visibility and control over large, complex AI systems in production. Both are highly valuable but serve different roles depending on your AI project's needs.\",\n",
      "            \"Prior to exploring Langsmith, it is crucial to differentiate between Langsmith and Langchain. While Langchain is mainly used for prototyping, Langsmith is designed to tackle challenges at the production level. Langsmith introduces new features centered around debugging, testing, evaluating, monitoring, and usage metrics.\\nKey Features of Langsmith\\nLangsmith provides various features to enhance the development and deployment of language models: [...] LangSmith helps your team debug, evaluate, and monitor your language models\\nLangsmith is an application framework for language models equipped with LLM tools and agent support. It focuses on addressing challenges faced during language model development for production. This guide offers an in-depth look into the essential features and functionalities of Langsmith.\\nUnderstanding Langsmith vs Langchain\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"generate\": {\n",
      "        \"draft\": \"**Title: Understanding the Differences Between LangChain and LangSmith**\\n\\n**I. Introduction**\\n\\nIn the rapidly evolving world of technology, language processing tools have become indispensable. As artificial intelligence (AI) and natural language processing (NLP) continue to advance, the demand for robust frameworks to manage and optimize language models has surged. Among the myriad of tools available, LangChain and LangSmith stand out as two prominent solutions, each catering to different aspects of language model development and deployment. This essay delves into the unique features and applications of LangChain and LangSmith, providing a comprehensive understanding of their roles in the AI landscape.\\n\\n**II. Overview of LangChain**\\n\\n**A. Definition and Purpose**\\n\\nLangChain is a versatile framework designed to facilitate the creation and orchestration of large language model (LLM) applications. Its primary objective is to streamline the development process for developers, enabling them to build complex NLP workflows with ease. LangChain plays a crucial role in AI by providing a flexible and intuitive platform for prototyping and deploying LLM-based applications.\\n\\n**B. Key Features**\\n\\nLangChain boasts several key features that make it a preferred choice for developers. Its architecture supports multi-step pipelines and agent-based systems, allowing for modular design and seamless integration with other systems. The framework's orchestration capabilities enable dynamic load management and model switching, which are essential for maintaining performance in high-traffic environments.\\n\\n**C. Use Cases**\\n\\nLangChain is commonly used in industries that require efficient NLP solutions, such as customer service, content generation, and decision-making systems. Its effectiveness in managing and scaling model workflows makes it ideal for applications ranging from text summarization to complex decision support systems.\\n\\n**III. Overview of LangSmith**\\n\\n**A. Definition and Purpose**\\n\\nLangSmith, on the other hand, is a framework tailored for the production-level challenges of language model deployment. Its primary objective is to provide deep visibility and control over large, complex AI systems. Unlike LangChain, which focuses on building workflows, LangSmith is designed for refining and optimizing deployed models.\\n\\n**B. Key Features**\\n\\nLangSmith offers advanced features for debugging, monitoring, and evaluating LLM workflows. Its architecture is equipped with tools for tracing, error handling, and performance monitoring, making it a precision toolkit for data scientists. LangSmith's unique selling points include its ability to provide detailed usage metrics and facilitate the optimization of model performance in production environments.\\n\\n**C. Use Cases**\\n\\nLangSmith is particularly effective in industries where the reliability and efficiency of language models are critical, such as finance, healthcare, and autonomous systems. Its capabilities in debugging and performance evaluation make it indispensable for applications that require rigorous testing and monitoring.\\n\\n**IV. Comparative Analysis**\\n\\n**A. Core Differences**\\n\\nThe core difference between LangChain and LangSmith lies in their primary use cases. LangChain is designed for building and orchestrating LLM workflows, while LangSmith focuses on debugging, monitoring, and evaluating these workflows. This distinction is reflected in their architecture, with LangChain offering a more modular design and LangSmith providing advanced diagnostic tools.\\n\\n**B. Performance and Efficiency**\\n\\nIn terms of performance, LangChain is generally lighter on hardware and computing resources, making it cost-effective for cloud environments. LangSmith, however, can be more resource-intensive due to its comprehensive monitoring and evaluation capabilities. Both tools excel in their respective domains, with LangChain prioritizing workflow management and LangSmith emphasizing system diagnostics.\\n\\n**C. User Experience and Accessibility**\\n\\nLangChain offers a moderate learning curve, requiring users to understand LLMs, while LangSmith is more accessible to those familiar with LLM pipelines. Both tools have strong community support, but LangChain's intuitive setup makes it more user-friendly for developers starting from scratch.\\n\\n**V. Advantages and Disadvantages**\\n\\n**A. LangChain**\\n\\nAdvantages of LangChain include its flexibility, scalability, and ease of use for developers. However, its focus on workflow management may limit its capabilities in production-level diagnostics.\\n\\n**B. LangSmith**\\n\\nLangSmith's strengths lie in its advanced debugging and monitoring features, making it ideal for optimizing deployed models. Its potential drawbacks include higher resource consumption and a steeper learning curve for those unfamiliar with LLM pipelines.\\n\\n**VI. Conclusion**\\n\\nIn summary, LangChain and LangSmith serve distinct yet complementary roles in the AI ecosystem. LangChain is the go-to tool for developers building LLM applications from the ground up, while LangSmith excels in refining and optimizing these applications in production. The choice between the two depends on the specific needs and stage of the AI project. As language processing tools continue to evolve, understanding the unique capabilities of LangChain and LangSmith will be crucial for leveraging their full potential in various industries.\\n\\n**VII. References**\\n\\n- [Include a list of references and resources for further reading on LangChain and LangSmith.]\",\n",
      "        \"revision_number\": 2\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"reflect\": {\n",
      "        \"critique\": \"Your essay provides a clear and structured comparison between LangChain and LangSmith, two frameworks in the realm of language model development and deployment. The organization of the essay into sections such as Introduction, Overview, Comparative Analysis, and Conclusion is effective in guiding the reader through the content. However, there are several areas where the essay could be improved to enhance its depth, clarity, and engagement.\\n\\n### Critique and Recommendations:\\n\\n1. **Introduction:**\\n   - **Depth and Context:** The introduction sets the stage for the discussion but could benefit from more context about the current landscape of AI and NLP tools. Consider adding a brief overview of why language models are important and how they are transforming industries.\\n   - **Thesis Statement:** While the introduction outlines the purpose of the essay, a more explicit thesis statement could help clarify the main argument or insight you wish to convey about LangChain and LangSmith.\\n\\n2. **Detailed Analysis:**\\n   - **Technical Details:** While you mention key features and use cases, the essay could be enriched with more technical details about how these frameworks operate. For instance, what specific technologies or algorithms do they employ? How do they integrate with existing AI ecosystems?\\n   - **Examples and Case Studies:** Incorporating specific examples or case studies of how LangChain and LangSmith have been used in real-world applications would provide practical insights and make the essay more engaging.\\n\\n3. **Comparative Analysis:**\\n   - **In-Depth Comparison:** The comparative analysis section could delve deeper into the nuances of how these frameworks differ in terms of user experience, scalability, and integration capabilities. Consider discussing any potential overlap in their functionalities and how they might complement each other in a single project.\\n   - **Performance Metrics:** Discussing specific performance metrics or benchmarks could provide a more quantitative comparison between the two frameworks.\\n\\n4. **User Experience and Accessibility:**\\n   - **User Feedback:** Including insights or feedback from developers or companies that have used these frameworks could add a valuable perspective on their accessibility and user experience.\\n\\n5. **Advantages and Disadvantages:**\\n   - **Balanced Viewpoint:** While you list advantages and disadvantages, ensure that both frameworks are evaluated with a balanced perspective. Consider potential future developments or updates that might address current limitations.\\n\\n6. **Conclusion:**\\n   - **Future Outlook:** The conclusion could be strengthened by discussing the future potential of these frameworks and how they might evolve with advancements in AI and NLP technologies.\\n\\n7. **References:**\\n   - **Comprehensive Sources:** Ensure that the references section includes a comprehensive list of sources, such as academic papers, industry reports, and official documentation, to support the claims made in the essay.\\n\\n8. **Style and Language:**\\n   - **Engagement:** Consider using more engaging language and varied sentence structures to maintain reader interest. Avoid overly technical jargon without explanation, as it might alienate readers unfamiliar with the subject.\\n\\n9. **Length and Depth:**\\n   - **Expansion:** The essay could be expanded to include more detailed sections on each framework, especially if you incorporate the additional technical details and examples suggested above. Aim for a balance between depth and readability.\\n\\nBy addressing these recommendations, you can enhance the essay's depth, clarity, and engagement, providing a more comprehensive and insightful analysis of LangChain and LangSmith.\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"research_critique\": {\n",
      "        \"content\": [\n",
      "            \"In fact, its orchestration capabilities allow for seamless model switching and dynamic load management, which can be critical when dealing with high-traffic environments where performance bottlenecks can be costly. In short, while LangChain excels at managing and scaling model workflows, LangSmith is designed for when you need deep visibility and control over large, complex AI systems in production. Both are highly valuable but serve different roles depending on your AI project's needs. [...] LangChain is designed for developers who are creating LLM-based applications from scratch. Its intuitive setup lets users quickly deploy LLM workflows without getting bogged down in complex configurations. Meanwhile, LangSmith was built so that data scientists could refine and optimize deployed workflows. These users typically understand how AI models work and can leverage LangSmith's advanced features to monitor, debug, and improve performance metrics in production. [...] Now, let's talk about resources because, in AI, efficiency can make or break your project. LangChain is typically lighter on hardware and computing resources. You don't need an overly complex setup to get it working, and in cloud environments, it's generally cost-effective since it focuses on managing workflows rather than deep system diagnostics. On the flip side, LangSmith can be more resource-intensive, especially in scenarios where you're monitoring and orchestrating multiple large models.\",\n",
      "            \"Why This Comparison Matters\\nHere\\u2019s the deal: LangChain and LangSmith aren\\u2019t just competing tools\\u2014they serve fundamentally different purposes. In my experience, LangChain is like a Swiss Army knife for building and orchestrating LLM applications. On the other hand, LangSmith is your precision toolkit when you need to debug, evaluate, and monitor those workflows. [...] Here\\u2019s the deal: if you\\u2019re designing and deploying workflows, LangChain is your foundation. It\\u2019s flexible, intuitive, and designed to help you build from the ground up. Personally, I\\u2019ve used it to create pipelines that span everything from summarization to decision-making. On the other hand, LangSmith shines when it\\u2019s time to refine. Debugging, monitoring, and evaluating\\u2014this is where LangSmith takes the spotlight. [...] | Learning Curve | Moderate: Requires understanding how LLMs work. | Minimal if you\\u2019re familiar with LangChain or LLM pipelines. |\\nDiving Deeper: Core Features\\nHere\\u2019s the deal: LangChain is like building the entire car, while LangSmith is your diagnostic tool to ensure that car runs smoothly.\\nLangChain: The Builder\\u2019s Dream\",\n",
      "            \"LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities. By understanding the differences, pros, and cons of each tool, developers can make informed decisions about which one to use for their specific needs.\",\n",
      "            \"Why This Comparison Matters\\nHere\\u2019s the deal: LangChain and LangSmith aren\\u2019t just competing tools\\u2014they serve fundamentally different purposes. In my experience, LangChain is like a Swiss Army knife for building and orchestrating LLM applications. On the other hand, LangSmith is your precision toolkit when you need to debug, evaluate, and monitor those workflows. [...] Here\\u2019s the deal: if you\\u2019re designing and deploying workflows, LangChain is your foundation. It\\u2019s flexible, intuitive, and designed to help you build from the ground up. Personally, I\\u2019ve used it to create pipelines that span everything from summarization to decision-making. On the other hand, LangSmith shines when it\\u2019s time to refine. Debugging, monitoring, and evaluating\\u2014this is where LangSmith takes the spotlight. [...] If you\\u2019re building from the ground up, LangChain is your best bet. It\\u2019s flexible, scalable, and designed for developers. But when it\\u2019s time to troubleshoot or improve your workflows, LangSmith steps in with tools to analyze, debug, and optimize.\\nFinal Note on Audiences\\nLangChain speaks to developers crafting solutions. LangSmith is a lifeline for those of us knee-deep in troubleshooting. Both are indispensable depending on where you are in your pipeline\\u2019s lifecycle.\",\n",
      "            \"LangChain vs. LangSmith: A Detailed Comparison\\nBoth tools are powerful, but they serve different purposes. Here's a breakdown of their core functionalities to help you understand which will help you the most.\\u00a0\\nFeature\\nLangChain\\nLangSmith\\nPrimary Use Case\\nBuilding and orchestrating complex LLM workflows.\\nDebugging, monitoring, and evaluating LLM workflows.\\nStrengths\\nMulti-step pipelines, agent-based systems, modular design.\\nTracing, error handling, performance monitoring, and evaluation metrics. [...] LangChain is designed for developers who are creating LLM-based applications from scratch. Its intuitive setup lets users quickly deploy LLM workflows without getting bogged down in complex configurations. Meanwhile, LangSmith was built so that data scientists could refine and optimize deployed workflows. These users typically understand how AI models work and can leverage LangSmith's advanced features to monitor, debug, and improve performance metrics in production. [...] In fact, its orchestration capabilities allow for seamless model switching and dynamic load management, which can be critical when dealing with high-traffic environments where performance bottlenecks can be costly. In short, while LangChain excels at managing and scaling model workflows, LangSmith is designed for when you need deep visibility and control over large, complex AI systems in production. Both are highly valuable but serve different roles depending on your AI project's needs.\",\n",
      "            \"Prior to exploring Langsmith, it is crucial to differentiate between Langsmith and Langchain. While Langchain is mainly used for prototyping, Langsmith is designed to tackle challenges at the production level. Langsmith introduces new features centered around debugging, testing, evaluating, monitoring, and usage metrics.\\nKey Features of Langsmith\\nLangsmith provides various features to enhance the development and deployment of language models: [...] LangSmith helps your team debug, evaluate, and monitor your language models\\nLangsmith is an application framework for language models equipped with LLM tools and agent support. It focuses on addressing challenges faced during language model development for production. This guide offers an in-depth look into the essential features and functionalities of Langsmith.\\nUnderstanding Langsmith vs Langchain\",\n",
      "            \"Generative AI Agents:\\nCurrent development focuses on sophisticated problem-solving capabilities, supported by declining costs and enhanced functionalities. These agents excel in natural language understanding, creative content generation, and human-like interactions. The 2025 landscape features advanced natural language processing, image generation, and multimodal AI systems, enabling more sophisticated cross-industry applications. [...] Specific advancements in natural language processing (NLP) include enhanced contextual understanding through transformer models, nuanced sentiment analysis, improved machine translation with near-human accuracy, advanced conversational AI with better dialogue management, and integration with other AI fields for more comprehensive applications. In image generation, enhanced Generative Adversarial Networks (GANs) produce images indistinguishable from real photos, improved text-to-image models [...] Moreover, AI agents are continuously improving in handling the challenges of understanding context and nuances in human language. Advances in NLP, contextual understanding, sentiment analysis, and conversational agents have led to more human-like interactions and accurate responses. These improvements are crucial for AI agents to perform effectively across various industries, enhancing their efficiency and effectiveness in specific tasks\\nHow AI Agents Will Change the Industry\",\n",
      "            \"As the tech industry continues to evolve, NLP and AI will play a crucial role in shaping the future of language and communication. Understanding AI and NLP. AI,\",\n",
      "            \"LangChain offers evaluation suite, LangSmith, tools for testing, debugging, and optimizing LLM applications, ensuring that applications perform well under real-world conditions.\\nConclusion\\nWhile both frameworks support integration with external tools and services, their primary focus areas set them apart.\\nLangChain is highly modular and flexible, focusing on creating and managing complex sequences of operations through its use of chains, prompts, models, memory, and agents. [...] In LangChain, an agent acts using natural language instructions and can use tools to answer queries. Based on user input, agents determine which actions to take and in what order. Actions can involve using tools (like a search engine or calculator) and processing their outputs or returning responses to users.\\nAgents can dynamically call chains based on user input.\\nLangChain Integrations: LangSmith and LangServe\\nLangSmith\",\n",
      "            \"debug, test, evaluate, and monitor chains and intelligent agents built\\n         on any LLM framework. LangSmith seamlessly integrates with LangChain's\\n          open-source framework called LangChain, which is widely used for \\n          building applications with LLMs.\\\\n\\\\nLangSmith provides full visibility \\n          into model inputs and outputs at every step in the chain of events, \\n          making it easier to debug and analyze the behavior of LLM applications. [...] LangSmith builds on LangChain, focusing on production readiness, while LangChain handles prototyping. The tracing tools in LangChain are indispensable for debugging and comprehending the execution steps of an agent, offering a visual representation of the sequence of calls within a workflow. This facilitates a deeper understanding of the model\\u2019s decision-making process, thereby fostering greater confidence in its accuracy.\",\n",
      "            \"4. Hands-on Demonstration\\nWhen it comes to practical application, I always say: \\u201cShow, don\\u2019t tell.\\u201d I\\u2019ve used both LangChain and LangSmith extensively, and I\\u2019ve found that they complement each other beautifully when you\\u2019re building and fine-tuning LLM-based workflows. Let me walk you through two real-world scenarios to illustrate their unique strengths.\\nExample Task 1: Building a Multi-step LLM Pipeline with LangChain [...] In one of my earlier projects, I needed to create a customer support bot capable of handling context across multiple questions. LangChain became my go-to because of its ability to string together multiple tasks seamlessly. Whether it was fetching data, performing summarizations, or interacting with APIs, LangChain handled it all in a modular fashion.\\nHere\\u2019s a real-world example to give you an idea:\\n```\\nfrom langchain.chains import ConversationChain [...] One example that comes to mind is when I built a pipeline using LangChain to analyze customer feedback. Everything seemed fine until I noticed inconsistencies in sentiment analysis results. LangSmith helped me trace the issue to a poorly constructed chain, saving me hours of trial-and-error debugging.\\nFinal Recommendation\",\n",
      "            \"They used both LangChain and LangSmith to accelerate development and enhance the quality of their AI-powered products. The integration allowed Elastic AI Assistant to manage complex workflows and deliver a superior product experience to their customers highlighting the impact of LangChain in real-world applications to streamline operations and optimize performance. [...] Both frameworks offer unique strengths, and understanding these can help developers align their needs with the right tool, leading to the construction of more efficient, powerful, and accurate LLM-powered applications.\\nRead more about the role of LlamaIndex and LangChain in orchestrating LLMs\\nReal-World Examples and Case Studies\\nLet\\u2019s look at some examples and use cases of LangChain in today\\u2019s digital world.\\nCustomer Service\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"generate\": {\n",
      "        \"draft\": \"**Title: Understanding the Differences Between LangChain and LangSmith**\\n\\n**I. Introduction**\\n\\nIn the rapidly evolving world of technology, language processing tools have become indispensable. As artificial intelligence (AI) and natural language processing (NLP) continue to advance, the demand for robust frameworks to manage and optimize language models has surged. Among the myriad of tools available, LangChain and LangSmith stand out as two prominent solutions, each catering to different aspects of language model development and deployment. This essay delves into the unique features and applications of LangChain and LangSmith, providing a comprehensive understanding of their roles in the AI landscape.\\n\\n**II. Overview of LangChain**\\n\\n**A. Definition and Purpose**\\n\\nLangChain is a versatile framework designed to facilitate the creation and orchestration of large language model (LLM) applications. Its primary objective is to streamline the development process for developers by offering an intuitive setup that allows for the rapid deployment of LLM workflows. LangChain plays a crucial role in NLP and AI by enabling the construction of complex, multi-step pipelines that can handle a variety of tasks, from summarization to decision-making.\\n\\n**B. Key Features**\\n\\nLangChain's architecture is characterized by its modular design, which supports multi-step pipelines and agent-based systems. This flexibility allows developers to build and manage intricate sequences of operations, integrating seamlessly with other systems. LangChain's orchestration capabilities enable seamless model switching and dynamic load management, making it an ideal choice for high-traffic environments where performance bottlenecks can be costly.\\n\\n**C. Use Cases**\\n\\nLangChain is commonly used in industries that require the development of LLM-based applications from scratch. Its effectiveness in various NLP tasks is evident in applications such as customer support bots, where it can string together multiple tasks seamlessly. By providing a foundation for designing and deploying workflows, LangChain has become a go-to tool for developers seeking to build robust AI solutions.\\n\\n**III. Overview of LangSmith**\\n\\n**A. Definition and Purpose**\\n\\nLangSmith, on the other hand, is a framework tailored for refining and optimizing deployed LLM workflows. It is designed for data scientists and AI practitioners who require deep visibility and control over large, complex AI systems in production. LangSmith's primary objective is to enhance the performance of language models by offering advanced debugging, testing, and monitoring capabilities.\\n\\n**B. Key Features**\\n\\nLangSmith's architecture is built around its unique selling points, which include tracing, error handling, performance monitoring, and evaluation metrics. These features provide full visibility into model inputs and outputs at every step in the chain of events, making it easier to debug and analyze the behavior of LLM applications. LangSmith's resource-intensive nature is justified by its ability to monitor and orchestrate multiple large models, ensuring optimal performance in production environments.\\n\\n**C. Use Cases**\\n\\nLangSmith is commonly used in industries that require production-ready applications with advanced debugging and monitoring capabilities. Its effectiveness in various NLP tasks is demonstrated in scenarios where deep system diagnostics are necessary to refine and optimize workflows. By providing tools to analyze, debug, and optimize, LangSmith has become an indispensable tool for those involved in troubleshooting and improving AI systems.\\n\\n**IV. Comparative Analysis**\\n\\n**A. Core Differences**\\n\\nThe core difference between LangChain and LangSmith lies in their primary use cases. LangChain is designed for building and orchestrating complex LLM workflows, while LangSmith focuses on debugging, monitoring, and evaluating these workflows. LangChain's modular design and flexibility make it ideal for early-stage prototyping, whereas LangSmith's advanced features cater to large-scale, production-ready applications.\\n\\n**B. Performance and Efficiency**\\n\\nIn terms of performance and efficiency, LangChain is typically lighter on hardware and computing resources, making it cost-effective in cloud environments. LangSmith, however, can be more resource-intensive due to its comprehensive monitoring and orchestration capabilities. Both tools excel in their respective domains, with LangChain offering scalability and LangSmith providing precision in performance optimization.\\n\\n**C. User Experience and Accessibility**\\n\\nLangChain offers a moderate learning curve, requiring an understanding of LLMs, while LangSmith is more accessible to those familiar with LangChain or LLM pipelines. Both tools provide user-friendly interfaces, but LangSmith's advanced features may require a deeper understanding of AI models. Community support for both tools is robust, ensuring that users can access resources and assistance as needed.\\n\\n**V. Advantages and Disadvantages**\\n\\n**A. LangChain**\\n\\nAdvantages of LangChain include its flexibility, scalability, and intuitive setup, making it ideal for developers building LLM-based applications. However, its focus on managing workflows rather than deep system diagnostics may limit its applicability in production environments.\\n\\n**B. LangSmith**\\n\\nLangSmith's advantages lie in its advanced debugging, testing, and monitoring capabilities, which are essential for optimizing production-ready applications. Its resource-intensive nature, however, may pose challenges in terms of hardware and computing requirements.\\n\\n**VI. Conclusion**\\n\\nIn summary, LangChain and LangSmith serve distinct yet complementary roles in the development and deployment of language models. LangChain is the foundation for building and orchestrating LLM applications, while LangSmith provides the tools necessary for refining and optimizing these workflows. By understanding the differences between these two frameworks, developers can make informed decisions about which tool best suits their specific needs and industry requirements. As language processing tools continue to evolve, both LangChain and LangSmith will play pivotal roles in shaping the future of AI and NLP.\\n\\n**VII. References**\\n\\n- Include a list of references and resources for further reading on LangChain and LangSmith.\",\n",
      "        \"revision_number\": 3\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for s in graph.stream({\n",
    "    'task': \"what is the difference between langchain and langsmith\",\n",
    "    \"max_revisions\": 2,\n",
    "    \"revision_number\": 1,\n",
    "    \"content\": []\n",
    "}, thread):\n",
    "     print(json.dumps(s, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
